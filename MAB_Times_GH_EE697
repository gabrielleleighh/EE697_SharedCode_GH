#packages that are needed to move forward with the algorithms GH

using StatsBase  #used for weight sampling
using Random #needed for making non stationary data
using Statistics #mean, median, etc , used for plotting option and avg  rewards
using Dates	# for the timing
using LinearAlgebra  # needed to normalize our data

# KL Divergence Algorithm ( what is the difference between two probability distributions?) 
function kl_divergence(p::Vector{Float64}, q::Vector{Float64})
    ε = 1e-8
    p = p .+ ε
    q = q .+ ε
    return sum(p .* log.(p ./ q))
end
 

# this will be called upon earlier to detect any shifts by comparing how state distributions evolve over time

# Defines the  Q-Learning Bandit Struct 
mutable struct QBandit
    n_arms::Int
    n_states::Int
    q_table::Matrix{Float64}
    alpha::Float64
    gamma::Float64
    epsilon::Float64
end
#basic q learning algorithm, we can call it the “set up”, “configure”, “defines”


function QBandit(n_arms::Int, n_states::Int; alpha=0.01, gamma=0.2, epsilon=0.1)
    q_table = zeros(n_states, n_arms)
    return QBandit(n_arms, n_states, q_table, alpha, gamma, epsilon)
end

# Q Learning Logic
function select_arm(bandit::QBandit, state::Int)
    if rand() < bandit.epsilon
        return rand(1:bandit.n_arms)
    else
        return argmax(bandit.q_table[state, :])
    end
end

#updates the q table 

function update!(bandit::QBandit, state::Int, arm::Int, reward::Float64, next_state::Int)
    best_next = maximum(bandit.q_table[next_state, :])
    bandit.q_table[state, arm] += bandit.alpha * (reward + bandit.gamma * best_next - bandit.q_table[state, arm])
end

#Simulates the Covariate Shift in the State Distribution

#Simulates a sudden shift in the distribution of states (inputs) at about halfway through


function simulate_covariate_shift(rounds, n_states)
    state_probs = fill(1.0 / n_states, n_states)
    state_history = Vector{Int}()
    prob_history = []

    for t in 1:rounds
        if t == Int(rounds * 0.5)
            # Introduce covariate shift (change input distribution)
            state_probs = normalize(rand(n_states), 1)
            println("Covariate shift detected at round:", t)
        end

        push!(prob_history, copy(state_probs))
        state = sample(1:n_states, Weights(state_probs))
        push!(state_history, state)
    end

    return state_history, prob_history
end

# main learning loop with shift detection
# this is the start of the training loop, all parameters may be changed to improve accuracy , delta timing, and how granular you want to be

function run_q_mab_with_covariate_shift()
    n_arms = 5
    n_states = 10
    rounds = 500   # this shiuld be changed to see if it recognizes a change at the "right round"
    bandit = QBandit(n_arms, n_states)

    state_history, prob_history = simulate_covariate_shift(rounds, n_states)
    total_rewards = Float64[]
    correct_preds = 0

    println("Training started...")
    start_time = time()

    for t in 1:rounds
        state = state_history[t]
        arm = select_arm(bandit, state)

        correct_arm = mod(state + t, n_arms) + 1  

#if the selected arm is correct, there's a high chance (up to 1.0) of reward.

        reward = rand() < (0.6 + 0.4 * (arm == correct_arm)) ? 1.0 : 0.0
	#Introduces non-stationarity into the reward pattern (psst this is dependent on both #state and time)


        if arm == correct_arm
            correct_preds += 1
        end

        next_state = rand(1:n_states)
        update!(bandit, state, arm, reward, next_state)
        push!(total_rewards, reward)

        # KL-based covariate shift detection
        if t > 10 && t % 10 == 0 # every 10 rounds, this will check how much the input distribution #has changed 
            p = prob_history[t - 10]
            q = prob_history[t]
            kl = kl_divergence(p, q)
            if kl > 0.5
                println("Covariate shift flagged at round $t with KL divergence = $(round(kl, digits=3))")
            end
        end
    end

    elapsed = time() - start_time

    println("Training complete.")
    println("Average reward: ", mean(total_rewards))
    println("Accuracy: ", correct_preds / rounds)
    println("Elapsed time (seconds): ", round(elapsed, digits=4))
end

run_q_mab_with_covariate_shift()

